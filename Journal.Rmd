---
title: "Estimación por Método de Variables Instrumentales - 2SLS"
output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: false
    toc_depth: 1
    #code_folding: hide
---
<style type="text/css">
.main-container {
  max-width: 90%;
  margin-left: auto;
  margin-right: auto;
}
body {
  text-align: justify;
}
</style>

```{r setup, include=FALSE}
library(mvtnorm)
library(AER)
library(ggplot2)
library(ERSA)
library(ggthemes)
library(dplyr)
library(kableExtra)
library(readr)
library(summarytools)
library(stargazer)
library(shiny)
library(doBy)
library(shinycustomloader)
library(sass)
knitr::opts_chunk$set(echo = FALSE)
set.seed(12345)
```

En este tutorial, procederemos a simular una variable instrumental para evaluar su efectividad al ser utilizada en una estimación de 2SLS. 

# Endogeneidad
Lo primero que haremos será simular un escenario de dos ecuaciones, que tendrá variables exógenas (generadas por fuera del modelo) y variables endógenas (generadas dentro del modelo). Para eso, pensemos en el siguiente par de ecuaciones (por simplicidad se omite el término constante):

$$  Y_i = \beta X_i + \lambda Q_i + u_i \hspace{0.3 cm} (1)$$


$$ X_i = \gamma Z_i + \delta Q_i + v_i \hspace{0.3 cm} (2) $$


Vemos que la variable $X_i$ se genera en la ecuación (2) y es a la vez una variable independiente en la ecuación (1). Definiremos los valores de  $\beta = 5$, y de $\lambda = \gamma = \delta =1$ para estudiar su estimación posterior.

Podemos considerar que tanto $Q_i$ como $Z_i$ son variables exógenas, y para efecto de este ejercicio, las simularemos con una distribución normal de 300 observaciones (ese sería el tamaño de la muestra en cada escenario). 

```` {r paso2, echo = TRUE}
Q <- rnorm(300,0,1)
Z <- rnorm(300,0,1)
````

Para construir la endogeneidad en $X_i$, simularemos ambos errores no observables ($u_i$, $v_i$), en base a una misma distribución normal bivariada, con una correlación entre -1 y 1. Al presentar esta correlación entre los errores, en la ecuación (1) la variable $X_i$ (generada en base al error $v_i$) se encuentra correlacionada con el error no observable de la ecuación ($u_i$).

Al estimar con OLS esta situación, es esperable que entregue un estimador sesgado, es decir, que su valor esperado no esté en el valor real (por construcción $\beta = 5$). Para evaluar esto, correremos 400 simulaciones del caso recién descrito, estimando cada vez $(\beta_{OLS})$. Observa que ocurre en el gráfico. 


# **Variables Instrumentales** 

## La eficiencia limpiando el sesgo.

Siguiendo con la simulación de la sección anterior, ya notamos que existe un sesgo en el estimador de OLS, que depende en gran medida de la magnitud de la correlación entre el regresor endógeno ($X_i$) y el error no observable.

$$ X_i = \gamma Z_i + \delta Q_i + v_i \hspace{0.3 cm} (2) $$

$$  Y_i = \beta X_i + \lambda Q_i + u_i \hspace{0.3 cm} (1)$$

Ahora probaremos utilizando la variable $Z_i$ como instrumento de la variable $X_i$. En nuestra simulación, $Z$ fue contruida de forma completamente exógena. Además, fue parte de la definición de $X_i$, por lo que cumple también con ser un instrumento relevante. 

La función que utilizaremos para realizar 2SLS será:

```` {r paso5, echo = TRUE}
# reg_vi <- ivreg(Y ~ X + Q | Z + Q)
````

Esta vez, nos concentraremos en la varianza de nuestro estimador. La simulación genera 400 iteraciones de una mismo escenario. Veremos cómo el tamaño de los datos de cada escenario afectan a la precisión del estimador. Al mirar el gráfico se puede ver que, aunque la varianza cambie, el promedio o valor medio siempre está en torno al valor 5, que es, el valor definido del parámetro $\beta = 5$.





# **Variable Instrumental Artificial** 

## Creación de una variable instrumental

Ahora, para cada una de las 400 iteraciones, simularemos variables $Za_i$ hasta encontrar una que esté correlacionada con la variable endógena $X_i$, a través del siguiente código. Una vez se escoge $Za_i$ que satisface la condición, se estima la regresión y se guarda el coeficiente.

```` {r paso8, echo = TRUE}
#    significativo <- 0
#    while (significativo == 0) {                         #Se repite el loop hasta que sea siginifcativo
#      Za <- rnorm(300,0,1)                               #Esta función genera un vector normal de 300 datos
#      reg3 <-lm(X~Za)                                    #Esta función realiza una regresión lineal
#      b <-summary(reg3)$coefficients[2,1]                #Aquí y la siguiente línea se guardan coeficientes
#      se <-summary(reg3)$coefficients[2,2]             
#      if(abs(b)/se > 1.96){                              #Esta es la condición de significancia al 95%
#        reg4<-ivreg(Y~X+Q|Za+Q)                          #Si es significativo, se estima VI con esa variable
#        B_IVa[i]=summary(reg4)$coefficients[2,1]         #Se guarda el valor estimado para ver la distribución
#        sig <- 1                                         #Se corta el loop para esta iteración
#      }
#    }
````

Al realizar este ejercicio, se grafican los 400 valores del estimador $\beta_Za$, tal y como se muestra a continuación:




# **Análisis conjunto**

Utilizar un instrumento artificial no funciona, pues aunque cada iteración en que se escoge aquella variable $Za_i$ que sí es relevante (porque está correlacionada con $X_i$), no es posible asegurar que este NO esté correlacionado con el error no observado ($u_i$). Lo anterior se debe a que la exogeneidad del instrumento ($Cov(Za_i, u_i) = 0$) es imposible de construir ni de observar. Por lo tanto, para cada caso en que $Za_i$ es relevante, NO puede asegurarse que sea al mismo tiempo exógena en la misma medida. Van a ocurrir casos en que sí, van a ocurrir casos en que no, es por eso que el estimador sigue estando sesgado, y además, es poco eficiente.

Esto se puede ver en el gráfico que junta las tres distribuciones para los escenarios simulados: OLS, VI, VI-artificial. Analice los gráficos y responda a las preguntas.

