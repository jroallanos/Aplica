---
title: "Estimación por Método de Variables Instrumentales - 2SLS"
output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: false
    toc_depth: 1
    #code_folding: hide
---
<style type="text/css">
.main-container {
  max-width: 90%;
  margin-left: auto;
  margin-right: auto;
}
body {
  text-align: justify;
}
</style>

```{r setup, include=FALSE}
library(mvtnorm)
library(AER)
library(ggplot2)
library(ERSA)
library(ggthemes)
library(dplyr)
library(kableExtra)
library(readr)
library(summarytools)
library(stargazer)
library(shiny)
library(doBy)
library(shinycustomloader)
library(sass)
knitr::opts_chunk$set(echo = FALSE)
set.seed(12345)
```

En este tutorial, procederemos a simular una variable instrumental para evaluar su efectividad al ser utilizada en una estimación de 2SLS. 

# **Endogeneidad**
Lo primero que haremos será simular un escenario de dos ecuaciones, que tendrá variables exógenas (generadas por fuera del modelo) y variables endógenas (generadas dentro del modelo). Para eso, pensemos en el siguiente par de ecuaciones (por simplicidad se omite el término constante):

$$  Y_i = \beta X_i + \lambda Q_i + u_i \hspace{0.3 cm} (1)$$


$$ X_i = \gamma Z_i + \delta Q_i + v_i \hspace{0.3 cm} (2) $$


Vemos que la variable $X_i$ se genera en la ecuación (2) y es a la vez una variable independiente en la ecuación (1). Definiremos los valores de  $\beta = 5$, y de $\lambda = \gamma = \delta =1$ para estudiar su estimación posterior.

Podemos considerar que tanto $Q_i$ como $Z_i$ son variables exógenas, y para efecto de este ejercicio, las simularemos con una distribución normal de 300 observaciones (ese sería el tamaño de la muestra en cada escenario). 

```` {r paso2, echo = TRUE}
Q <- rnorm(300,0,1)
Z <- rnorm(300,0,1)
````

Para construir la endogeneidad en $X_i$, simularemos ambos errores no observables ($u_i$, $v_i$), en base a una misma distribución normal bivariada, con una correlación entre -1 y 1. Al presentar esta correlación entre los errores, en la ecuación (1) la variable $X_i$ (generada en base al error $v_i$) se encuentra correlacionada con el error no observable de la ecuación ($u_i$).

Al estimar con OLS esta situación, es esperable que entregue un estimador sesgado, es decir, que su valor esperado no esté en el valor real (por construcción $\beta = 5$). Para evaluar esto, correremos 400 simulaciones del caso recién descrito, estimando cada vez $(\beta_{OLS})$. Observa que ocurre en [el gráfico.](https://www.shinyapps.io)



# **Variables Instrumentales** 

## La eficiencia limpiando el sesgo.

Siguiendo con la simulación de la sección anterior, ya notamos que existe un sesgo en el estimador de OLS, que depende en gran medida de la magnitud de la correlación entre el regresor endógeno ($X_i$) y el error no observable.

$$ X_i = \gamma Z_i + \delta Q_i + v_i \hspace{0.3 cm} (2) $$

$$  Y_i = \beta X_i + \lambda Q_i + u_i \hspace{0.3 cm} (1)$$

Ahora probaremos utilizando la variable $Z_i$ como instrumento de la variable $X_i$. En nuestra simulación, $Z$ fue contruida de forma completamente exógena. Además, fue parte de la definición de $X_i$, por lo que cumple también con ser un instrumento relevante. 

La función que utilizaremos para realizar 2SLS será:

```` {r paso5, echo = TRUE, eval=FALSE}
reg_vi <- ivreg(Y ~ X + Q | Z + Q)
````

Esta vez, nos concentraremos en la varianza de nuestro estimador. La simulación genera 400 iteraciones de una mismo escenario. Veremos cómo el tamaño de los datos de cada escenario afectan a la precisión del estimador. Al mirar [el gráfico](https://www.shinyapps.io) se puede ver que, aunque la varianza cambie, el promedio o valor medio siempre está en torno al valor 5, que es, el valor definido del parámetro $\beta = 5$.





# **Variable Instrumental Artificial** 

## Creación de una variable instrumental

Ahora, para cada una de las 400 iteraciones, simularemos variables $Za_i$ hasta encontrar una que esté correlacionada con la variable endógena $X_i$, a través del siguiente código. Una vez se escoge $Za_i$ que satisface la condición, se estima la regresión y se guarda el coeficiente.

```` {r paso8, echo = TRUE, eval=FALSE}
significativo <- 0
while (significativo == 0) {                      #Se repite el loop hasta que sea siginifcativo
  Za <- rnorm(300,0,1)                            #Esta función genera un vector normal de 300 datos
  reg3 <-lm(X~Za)                                 #Esta función realiza una regresión lineal
  b <-summary(reg3)$coefficients[2,1]             #Aquí y la siguiente línea se guardan coeficientes
  se <-summary(reg3)$coefficients[2,2]             
  if(abs(b)/se > 1.96){                           #Esta es la condición de significancia al 95%
    reg4<-ivreg(Y~X+Q|Za+Q)                       #Si es significativo, se estima VI con esa variable
    B_IVa[i]=summary(reg4)$coefficients[2,1]      #Se guarda el valor estimado para ver la distribución
    sig <- 1                                      #Se corta el loop para esta iteración
  }
}
````

Al realizar este ejercicio, se grafican los 400 valores del estimador $\beta_Za$, tal y como se muestra a continuación:


```{r,echo=FALSE,warning=FALSE}
beta=5
lambda=1
gamma=1
delta=1
B_IVa <-rep(0,400) #se guardarán acá los betas_IV aleatorios si es que son significativos
for(i in 1:400){ #hacemos 400 simulaciones
  Q<-rnorm(300,0,1) #Una variable de distribución normal con 300 valores
  Z<-rnorm(300,0,1) #Una variable de distribución normal con 300 valores
  sigma <- matrix(c(1,0.85,0.85,1), ncol=2)
  epsilon <- rmvnorm(n=300, mean=c(0,0), sigma=sigma) #Este es el origen de la endogeneidad
  u=epsilon[,1] 
  e=epsilon[,2] 
  X=gamma*Z+delta*Q+e #Calculamos X en función de las simulaciones Z, Q y e
  Y=beta*X+lambda*Q+u #Calculamos Y en función de X y las simulaciones Q, y u
  sig <- 0
  while (sig == 0) {
    Za<-rnorm(300,0,1) 
    reg3<-lm(X~Za)
    b <-summary(reg3)$coefficients[2,1] 
    se <-summary(reg3)$coefficients[2,2] 
    if(abs(b)/se > 1.96){ 
      reg4<-ivreg(Y~X+Q|Za+Q)
      B_IVa[i]=summary(reg4)$coefficients[2,1]
      sig <- 1
    }
  }
}
 ggplot(as.data.frame(B_IVa)) + 
    aes(B_IVa)+
    geom_density(alpha=0.3, fill="#E69F00") +
    geom_vline(xintercept=5, linetype="dashed", color="red")+
    theme_fivethirtyeight() + 
    theme(panel.grid = element_blank(),
        axis.line = element_line(colour = "black"), 
        plot.title = element_text(hjust = 0.5,size=18), 
        text = element_text(size=18), 
        axis.title = element_text(),
        axis.text.x = element_text(angle=60, hjust=1,size=18)) + 
    xlim(3.5,6.5) + ylim(0,15) +
    labs(x="Estimador IV artificial", #agrega etiqueta del eje X
       y="Densidad", #agrega etiqueta del eje y
       title="Distribución estimador IV artificial")
```

# **Análisis conjunto**

Utilizar un instrumento artificial no funciona, pues aunque cada iteración en que se escoge aquella variable $Za_i$ que sí es relevante (porque está correlacionada con $X_i$), no es posible asegurar que este NO esté correlacionado con el error no observado ($u_i$). Lo anterior se debe a que la exogeneidad del instrumento ($Cov(Za_i, u_i) = 0$) es imposible de construir ni de observar. Por lo tanto, para cada caso en que $Za_i$ es relevante, NO puede asegurarse que sea al mismo tiempo exógena en la misma medida. Van a ocurrir casos en que sí, van a ocurrir casos en que no, es por eso que el estimador sigue estando sesgado, y además, es poco eficiente.

Esto se puede ver en el gráfico que junta las tres distribuciones para los escenarios simulados: OLS, VI, VI-artificial. Analice los gráficos y responda a las preguntas.

```{r}
#Se inicializan los indicadores de acuerdo a lo propuesto en las ecuaciones
beta=5
lambda=1
gamma=1
delta=1

nobs=300
nrep=400

betas1=matrix(data=NA,nrow=nrep,ncol=3)

for(i in 1:nrep){
  #generación de la distribución multivariada estandar, primera columna es "e" y segunda columan "u"
  Errores=rmvnorm(nobs,mean=c(0,0),sigma=matrix(c(1,0.8,0.8,1),2,2))
  #generación Q y Z
  Q=rnorm(nobs,mean=0,sd=1)
  Z=rnorm(nobs,mean=0,sd=1)
  #Hacer la variable X
  X=gamma*Z+delta*Q+Errores[,1]
  Y=beta*X+lambda*Q+Errores[,2]
  #Escenario OLS
  ols=lm(Y~X+Q)
  betas1[i,1]=ols$coefficients[2]
  #Escenario 2SLS 1
  sls<-ivreg(Y~X+Q|Z+Q) #Estimamos el valor de beta y de lambda de las ecuaciones mediante 2SLS
  betas1[i,2]=sls$coefficients[2]
  #Escenario 2SLS 2
  sig <- 0
  while (sig == 0) {
    Za=rnorm(nobs,mean=0,sd=1)
    fls2=lm(X~Za+Q)
    stat.coef  <- summary(fls2)$coefficients
    coef    <- stat.coef[,1]    #coeficientes 
    se.coef <- stat.coef[,2]    # error estandar para cada coef.
    if(abs(coef[2]/se.coef[2]) > 1.96){
      sig <- 1
      ra <- ivreg(Y~X+Q|Za+Q)
      betas1[i,3]=ra$coefficients[2]
    }
  }
  
}

betas1=data.frame(betas1)
names(betas1) = c("Beta-ols","Beta-iv","Beta-iva")
mediabetaols=round(mean(betas1[,1]),2)
mediabetaiv=round(mean(betas1[,2]),2)
mediabetaiva=round(mean(betas1[,3]),2)
betas1 <- stack(betas1)


ggplot(betas1, aes(x=values)) + 
  geom_density(aes(group=ind, fill=ind), alpha=0.3)+
  theme_fivethirtyeight()+ #tema reconocido de la página "fivethirtyeight"
  theme(panel.grid = element_blank(),
        axis.line = element_line(colour = "black"), 
        plot.title = element_text(hjust = 0.5,size=18), 
        text = element_text(size=18), 
        axis.title = element_text(),
        axis.text.x = element_text(angle=60, hjust=1,size=18), 
        legend.background = element_rect(fill="gray", size=0.5, linetype="solid", colour = "black"), 
        legend.text = element_text(size = 18), 
        legend.position = "right", legend.direction = "vertical")+ 
  scale_fill_manual(labels =  c("OLS","IV","IV artificial"),values=c("#56B4E9","#999999","#E69F00"))+
  labs(x="Estimador", #agrega etiqueta del eje X
       y="Densidad", #agrega etiqueta del eje y
       title="Distribución estimadores")+
  guides(fill=guide_legend(title="Distribución:")) + 
  xlim(3.5,6.5) + ylim(0,15)
```

